{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50827f80",
   "metadata": {
    "execution": {
     "execution_failed": "2025-06-24T14:06:00.861Z",
     "iopub.execute_input": "2025-06-24T14:05:23.609387Z",
     "iopub.status.busy": "2025-06-24T14:05:23.608904Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m218.5/218.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.4/491.4 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.4/152.4 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.5/31.5 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.2/865.2 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m219.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m^C:01\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m219.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q unsloth==2025.4.7 datasets==3.5.1\n",
    "\n",
    "# Set CUDA device and disable Triton\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" #chon GPU dau tien\n",
    "os.environ[\"TRITON_DISABLE\"] = \"1\"  # tat triton pytorch\n",
    "os.environ[\"TRITON_DISABLE_LINE_INFO\"] = \"1\" \n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from huggingface_hub import login, create_repo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e95af6-3ab5-410b-8a84-7d21198c7b07",
   "metadata": {
    "execution": {
     "execution_failed": "2025-06-24T14:06:00.861Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "login(token=\"hf_......................\")\n",
    "\n",
    "model_name = \"unsloth/Llama-3.1-8B-Instruct-bnb-4bit\" #tai base model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "#cau hình quantize\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,   # quantize 4 bit ---> giam 2^n (n=4)\n",
    "    bnb_4bit_quant_type=\"nf4\", #normal float 4-bit\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16 #dinh dang float 16-bit\n",
    ")\n",
    "\n",
    "#load model tu duong dan cuc bo\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16 #tensor se co dang float 16-bit\n",
    ")\n",
    "\n",
    "# cau hinh lora\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16, #scale factor\n",
    "    lora_dropout=0,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"down_proj\", \"up_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    use_rslora=True\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "dataset_name = \"5CD-AI/Vietnamese-Multi-turn-Chat-Alpaca\"\n",
    "hf_dataset = load_dataset(dataset_name, split=\"train\")\n",
    "local_dataset = load_dataset(\"json\", data_files=\"/kaggle/input/formate-data/formated_data.json\", split=\"train\") #du lieu o local\n",
    "\n",
    "SYS_INSTRUCT_FRIENDLY = \"Bạn là một trợ lý AI thân thiện, hãy trả lời bằng tiếng Việt.\"\n",
    "SYS_INSTRUCT_EXPERT = \"Bạn là một chuyên gia AI, hãy chuyển câu trong ngôn ngữ ký hiệu Việt Nam VSL sang ngôn ngữ nói.\"\n",
    "\n",
    "\n",
    "def convert_to_chat_format(conversations, is_expert=False):\n",
    "    #chon instruction cho tung nguon du lieu\n",
    "    sys_instruction = SYS_INSTRUCT_EXPERT if is_expert else SYS_INSTRUCT_FRIENDLY\n",
    "    messages = [{\"role\": \"system\", \"content\": sys_instruction}]\n",
    "    for msg in conversations:\n",
    "        role = \"user\" if msg[\"from\"] == \"human\" else \"assistant\"\n",
    "        messages.append({\"role\": role, \"content\": msg[\"value\"]})\n",
    "    return messages\n",
    "    \n",
    "\n",
    "def format_prompt(example, is_expert=False):\n",
    "    messages = convert_to_chat_format(example[\"conversations\"], is_expert=is_expert)\n",
    "    return {\n",
    "        \"text\": tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "    }\n",
    "\n",
    "# Áp dụng format_prompt cho từng dataset\n",
    "hf_dataset = hf_dataset.map(lambda x: format_prompt(x, is_expert=False))  \n",
    "local_dataset = local_dataset.map(lambda x: format_prompt(x, is_expert=True)) \n",
    "\n",
    "# Kết hợp dataset sau khi áp dụng instruction\n",
    "combined_dataset = concatenate_datasets([hf_dataset, local_dataset])\n",
    "\n",
    "\n",
    "def tokenize(example):\n",
    "    tokens = tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].clone()\n",
    "    return {k: v.squeeze() for k, v in tokens.items()}\n",
    "\n",
    "combined_dataset = combined_dataset.map(tokenize, batched=True, remove_columns=combined_dataset.column_names)\n",
    "\n",
    "#CustomTrainer\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "        attention_mask = inputs[\"attention_mask\"]\n",
    "        labels = inputs[\"labels\"]\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        loss = torch.nn.functional.cross_entropy(\n",
    "            logits.view(-1, logits.size(-1)),\n",
    "            labels.view(-1),\n",
    "            ignore_index=-100\n",
    "        )\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=2, \n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=1e-4,\n",
    "    warmup_ratio=0.05, #tang dan to do hoc 0.05*max steps = 50 lan tang\n",
    "    logging_steps=100, #ghi lai sau 100 buoc\n",
    "    save_strategy=\"steps\", #luu theo step\n",
    "    save_steps=50,\n",
    "    output_dir=\"./llama3-chat-t4\",\n",
    "    save_total_limit=2,\n",
    "    max_steps=1000, \n",
    "    report_to=\"none\",\n",
    "    fp16=True,\n",
    "    bf16=False, #la gia tri mac dinh, rong hon, on dinh voi gradient lon\n",
    ")\n",
    "#Khởi tạo trainer và huấn luyện\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=combined_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# 8. Merge LoRA và push lên Hugging Face\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Tạo repository mới trên Hugging Face\n",
    "repo_name = \"VyDat/llama3-8b-vietnamese-multi\"\n",
    "create_repo(repo_id=repo_name, private=False, exist_ok=True)\n",
    "\n",
    "# Push mô hình và tokenizer\n",
    "model.push_to_hub(repo_name, commit_message=\"Merge & push fine-tuned LLaMA3-8B\")\n",
    "tokenizer.push_to_hub(repo_name, commit_message=\"Push tokenizer\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7713852,
     "sourceId": 12242651,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
